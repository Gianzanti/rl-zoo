# Hyperparameter tunning with Optuna
# DarwinOp3-v0 -> v.0.2.2
# 2025/02/10
# keep_alive_reward: 0.5

# Trial 299 finished with value: 758.1771902 and parameters: {'batch_size': 32, 'n_steps': 512, 'gamma': 0.9999, 'learning_rate': 5.2179148371957376e-05, 'ent_coef': 0.00015001834201324322, 'clip_range': 0.3, 'n_epochs': 20, 'gae_lambda': 0.95, 'max_grad_norm': 0.5, 'vf_coef': 0.1169892059401605, 'net_arch': 'medium', 'log_std_init': -2.7793964015976154, 'sde_sample_freq': 128, 'activation_fn': 'relu', 'lr_schedule': 'linear'}. Best is trial 299 with value: 758.1771902.


DarwinOp3-v0:
  # normalize: true
  policy: 'MlpPolicy'
  n_timesteps: !!float 5e7
  n_envs: 6
  callback: 
    - research.callbacks.tb_cb.TensorboardCallback

  batch_size: 32
  n_steps: 512
  gamma: 0.9999
  learning_rate: lin_5.2179148371957376e-05
  ent_coef: 0.00015001834201324322
  clip_range: 0.3
  n_epochs: 20
  gae_lambda: 0.98
  max_grad_norm: 0.5
  vf_coef : 0.1169892059401605
  sde_sample_freq: 128
  policy_kwargs: "dict(
    log_std_init=-2.7793964015976154,
    ortho_init=False,
    activation_fn=nn.ReLU,
    net_arch=dict(pi=[256, 256], vf=[256, 256])
  )"




# net_arch = {
#     "tiny": dict(pi=[64], vf=[64]),
#     "small": dict(pi=[64, 64], vf=[64, 64]),
#     "medium": dict(pi=[256, 256], vf=[256, 256]),
# }[net_arch_type]