# Hyperparameter tunning with Optuna
# DarwinOp3-v0 -> v.0.2.2
# 2025/02/10
# keep_alive_reward: 0.5

# Trial 299 finished with value: 758.1771902 and parameters: {'batch_size': 32, 'n_steps': 512, 'gamma': 0.9999, 'learning_rate': 5.2179148371957376e-05, 'ent_coef': 0.00015001834201324322, 'clip_range': 0.3, 'n_epochs': 20, 'gae_lambda': 0.95, 'max_grad_norm': 0.5, 'vf_coef': 0.1169892059401605, 'net_arch': 'medium', 'log_std_init': -2.7793964015976154, 'sde_sample_freq': 128, 'activation_fn': 'relu', 'lr_schedule': 'linear'}. Best is trial 299 with value: 758.1771902.



# Hyperparameter tunning with Optuna
# DarwinOp3-v1 -> v.0.2.13
# 2025/02/15
# keep_alive_reward: 1.0
# _motor_max_torque = 3.0

# Trial 20 finished with value: 85.0 and parameters: {'batch_size': 256, 'n_steps': 256, 'gamma': 0.9999, 'learning_rate': 0.0006627047899979045, 'ent_coef': 1.2158492690967722e-06, 'clip_range': 0.1, 'n_epochs': 20, 'gae_lambda': 1.0, 'max_grad_norm': 0.6, 'vf_coef': 0.6657206046663933, 'net_arch': 'small', 'log_std_init': 0.9801114932717399, 'sde_sample_freq': 8, 'activation_fn': 'tanh', 'lr_schedule': 'constant'}. Best is trial 20 with value: 85.0.

# Trial 44 finished with value: 92.6 and parameters: {'batch_size': 16, 'n_steps': 128, 'gamma': 0.9999, 'learning_rate': 2.1377024097168636e-05, 'ent_coef': 3.2889468687038134e-07, 'clip_range': 0.4, 'n_epochs': 20, 'gae_lambda': 0.95, 'max_grad_norm': 0.7, 'vf_coef': 0.811547194324299, 'net_arch': 'medium', 'log_std_init': -0.48830896488074915, 'sde_sample_freq': 16, 'activation_fn': 'tanh', 'lr_schedule': 'constant'}. Best is trial 44 with value: 92.6

# Trial 68 finished with value: 123.4 and parameters: {'batch_size': 64, 'n_steps': 256, 'gamma': 0.95, 'learning_rate': 0.00019132238529037985, 'ent_coef': 1.1059805712610445e-08, 'clip_range': 0.1, 'n_epochs': 20, 'gae_lambda': 0.92, 'max_grad_norm': 0.6, 'vf_coef': 0.6903251457496492, 'net_arch': 'small', 'log_std_init': 0.5008365225421337, 'sde_sample_freq': -1, 'activation_fn': 'tanh', 'lr_schedule': 'constant'}. Best is trial 68 with value: 123.4.

# Trial 78 finished with value: 441.4 and parameters: {'batch_size': 128, 'n_steps': 2048, 'gamma': 0.9999, 'learning_rate': 0.0007666339265330843, 'ent_coef': 6.825722044873898e-06, 'clip_range': 0.1, 'n_epochs': 20, 'gae_lambda': 0.95, 'max_grad_norm': 0.6, 'vf_coef': 0.7379865410033998, 'net_arch': 'medium', 'log_std_init': 0.6316775104624097, 'sde_sample_freq': 8, 'activation_fn': 'relu', 'lr_schedule': 'constant'}. Best is trial 78 with value: 441.4

DarwinOp3-v1:
  policy: 'MlpPolicy'
  n_timesteps: !!float 5e7
  n_envs: 8
  callback: 
    - gym_env.envs.callbacks.TensorboardCallback
  env_wrapper:
    - gymnasium.wrappers.TimeLimit:
        max_episode_steps: 3100
  # env_kwargs:
  #   motor_max_torque: 3.0
  #   keep_alive_reward: 0.1
  #   forward_velocity_weight: 3.0
  #   target_distance: 3.0
  #   reach_target_reward: 1000.0

  # batch_size: 128
  # n_steps: 2048
  # gamma: 0.9999
  # learning_rate: 0.0007666339265330843
  # ent_coef: 6.825722044873898e-06
  # clip_range: 0.1
  # n_epochs: 20
  # gae_lambda: 0.95
  # max_grad_norm: 0.6
  # vf_coef : 0.7379865410033998
  # sde_sample_freq: 8
  # policy_kwargs: "dict(
  #   log_std_init=0.6316775104624097,
  #   ortho_init=False,
  #   activation_fn=nn.ReLU,
  #   net_arch=dict(pi=[256, 256], vf=[256, 256])
  # )"

# net_arch = {
#     "tiny": dict(pi=[64], vf=[64]),
#     "small": dict(pi=[64, 64], vf=[64, 64]),
#     "medium": dict(pi=[256, 256], vf=[256, 256]),
# }[net_arch_type]

# activation_fn = {"tanh": nn.Tanh, "relu": nn.ReLU, "elu": nn.ELU, "leaky_relu": nn.LeakyReLU}[activation_fn_name]